{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f03ba44a-5feb-4bcc-90af-63e8b1f44bd5",
   "metadata": {},
   "source": [
    "## In this Notebook we look at how the different models can interact with each other based on the context. We give openAI, Claude and Gemini models personas and ask them to talk about a topic like Global Warming. It is a fun way to learn about the different models and how they can interact with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()\n",
    "\n",
    "googleAI = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## As a simple case prompts are organised into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "This structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "gemini_model = 'gemini-2.0-flash-exp'\n",
    "topic_to_discuss = \"Global Warming\"\n",
    "\n",
    "gpt_system = \"You are a chatbot named mini who is in a 3-way conversation with Haiku and Flash which will be a debate and argument. \\\n",
    "Your view point is conservative.  It can be even be described as 'far-right'. \\\n",
    "your thinking can be traditional at times. You will be given a topic to discuss. On the 5th round of discussion thank everyone,let everyone know who made the most convincing argument other than yourself.\"\n",
    "\n",
    "claude_system = \"You are a chatbot named haiku who is in a 3-way conversation with Mini and Flash which will be a debate and argument. \\\n",
    "You have a modern take on the world.  It can be even be described as 'far-left'. \\\n",
    "your thinking can be quite a shock to the traditionalists at times. You will be given a topic to discuss. On the 5th round of discussion thank everyone,let everyone know who made the most convincing argument other than yourself.\"\n",
    "\n",
    "gemini_system = \"You are a chatbot named flash who is in a 3-way conversation with Mini and Haiku which will be a debate and argument. \\\n",
    "Your view point is balanced.  It can be even be described as 'centred'. \\\n",
    "your thinking can be measured and practical. You will be given a topic to discuss. On the 5th round of discussion thank everyone,let everyone know who made the most convincing argument other than yourself.\"\n",
    "\n",
    "gpt_messages = [\"Hi there I'm Mini!\"]\n",
    "claude_messages = [\"Hi I'm Haiku\"]\n",
    "gemini_messages = [f\"Hi, I'm Flash. Lets discuss about {topic_to_discuss}.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt(system_prompt = gpt_system):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    for gpt, claude, gemini in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110f63ae-8d5f-43d8-859b-88cd2c6c19ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude(system_prompt = claude_system):\n",
    "    messages = []\n",
    "    \n",
    "    for gpt, claude_message, gemini in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gemini})\n",
    "    messages.append({\"role\": \"user\", \"content\": gemini_messages[-1]})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    \n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=system_prompt,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c180c2d6-df4d-42e3-8bac-4e95f1ec5d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini(system_prompt = gemini_system):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    for gpt, claude, gemini in zip(gpt_messages, claude_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gemini})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    messages.append({\"role\": \"user\", \"content\": claude_messages[-1]})\n",
    "    completion = googleAI.chat.completions.create(\n",
    "        model=gemini_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mini:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Haiku:\\n{claude_messages[0]}\\n\")\n",
    "print(f\"Flash:\\n{gemini_messages[0]}\\n\")\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"Mini:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "\n",
    "    claude_next = call_claude()\n",
    "    print(f\"Haiku:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)\n",
    "\n",
    "    gemini_next = call_gemini()\n",
    "    print(f\"Flash:\\n{gemini_next}\\n\")\n",
    "    gemini_messages.append(gemini_next)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
